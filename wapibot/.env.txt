# ============================================================================
# BACKEND V2 ENVIRONMENT VARIABLES
# ============================================================================

# Application
APP_NAME="WapiBot Backend V2"
APP_VERSION="2.0.0"
DEBUG=True
ENVIRONMENT="development"

# Server
HOST="0.0.0.0"
PORT=8000
RELOAD=True

# Database (SQLite for checkpoints)
DATABASE_URL="sqlite+aiosqlite:///./conversations.db"
CHECKPOINT_DB_PATH="./checkpoints.db"

# ============================================================================
# LLM PROVIDERS - Choose one as PRIMARY_LLM_PROVIDER
# ============================================================================
PRIMARY_LLM_PROVIDER="ollama"  # Options: "ollama", "openrouter", "openai"

# LLM Provider - Ollama (Local, Free, No API key needed)
OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_MODEL="gemma3:4b"
OLLAMA_TIMEOUT=30.0
OLLAMA_MAX_RETRIES=2

# LLM Extraction Timeouts (in seconds)
# These are realistic for RTX 3060 - adjust for your hardware
EXTRACTION_TIMEOUT_NORMAL=90.0        # Normal extraction (10-90s typical)
EXTRACTION_TIMEOUT_COMPLEX=180.0      # Complex modules or first load
EXTRACTION_TIMEOUT_WARMUP=180.0       # Warmup queries (model loading time)

# LLM Provider - OpenRouter (Paid, supports 100+ models with failover)
OPENROUTER_API_KEY=""
OPENROUTER_BASE_URL="https://openrouter.ai/api/v1"
OPENROUTER_MODEL="openai/gpt-4o-mini"
OPENROUTER_FALLBACK_MODEL="anthropic/claude-3.5-haiku"
OPENROUTER_APP_NAME="WapiBot"
OPENROUTER_SITE_URL="https://github.com/yourusername/wapibot"
OPENROUTER_TIMEOUT=60.0

# LLM Provider - OpenAI (Paid, direct)
OPENAI_API_KEY=""
OPENAI_MODEL="gpt-4o-mini"
OPENAI_TIMEOUT=30.0

# DSPy Configuration
DSPY_CACHE_DIR="./dspy_cache"
DSPY_MAX_TOKENS=2000
DSPY_TEMPERATURE=0.4

# Confidence Score Thresholds
CONFIDENCE_LOW=0.5                    # Low confidence extraction
CONFIDENCE_MEDIUM=0.7                 # Medium confidence (regex fallback)
CONFIDENCE_HIGH=0.9                   # High confidence (DSPy extraction)

# Warmup Configuration
WARMUP_COOLDOWN_SECONDS=15           # Don't warmup again within 5 minutes (hot reload protection)
WARMUP_IDLE_THRESHOLD_SECONDS=30     # Trigger warmup after 5 minutes idle
WARMUP_IDLE_CHECK_INTERVAL=60         # Check idle status every minute

# LangGraph Configuration
LANGGRAPH_CHECKPOINT_ENABLED=True
LANGGRAPH_CHECKPOINT_PATH="./checkpoints.db"

# Workflow Selection (hot-swappable - no code changes needed!)
# Options: "existing_user_booking", "marketing_to_registration", "v2_full_workflow"
ACTIVE_WORKFLOW="existing_user_booking"

# WAPI (WhatsApp Business API)
WAPI_BASE_URL="https://wapi.in.net/api/"
WAPI_VENDOR_UID="cd4203ff-71c9-4872-8682-41e835f8a250"  # Your WAPI vendor UID
WAPI_BEARER_TOKEN="bmCmGnTiHdoMS4nlPut0Kf6UNXoNzOdyLwkVpA2w5b2Pbs1NTjhGlQv8oV8C8T3P"  # Your WAPI bearer token for API authentication
WAPI_FROM_PHONE_NUMBER_ID="963043230216783"  # Optional: Default phone number ID
WAPI_WEBHOOK_SECRET=""  # Optional: HMAC secret for webhook signature validation
WAPI_WEBHOOK_URL="https://inconsumable-jennie-peskily.ngrok-free.dev/api/v1/wapi/webhook"  # Current ngrok webhook URL (update when ngrok restarts)

# Frappe Backend
FRAPPE_BASE_URL="https://yawlit.duckdns.org"
FRAPPE_API_KEY="57ebe42fbfc0dd0"
FRAPPE_API_SECRET="1858180856edc27"

# Yawlit Client
YAWLIT_BASE_URL="http://localhost:3000"
YAWLIT_API_KEY=""

# Frontend (NextJS Mock)
FRONTEND_URL="http://localhost:3000"
CORS_ORIGINS="http://localhost:3000,http://localhost:3001"

# Logging
LOG_LEVEL="INFO"
LOG_FORMAT="json"
LOG_FILE="./logs/backend.log"

# Security
SECRET_KEY="your-secret-key-change-in-production"
ALLOWED_HOSTS="*"

# Rate Limiting
RATE_LIMIT_ENABLED=False
RATE_LIMIT_PER_MINUTE=60

# Feature Flags
ENABLE_SENTIMENT_ANALYSIS=True
ENABLE_INTENT_CLASSIFICATION=True
ENABLE_BATCH_PROCESSING=True
